changeBg:N（主角相关工作地点）/N9.jpg -next
changeFigure:mygo_avemujica_v6/sakiko/341_casual-2023_rip/model.json -transform={"position":{"y":-250}} -animationFlag=on -motion=bye02 -expression=smile01 -next 
客服小祥:各位B站的朋友大家好。 -center
看到现在了应该没有人觉得这个是写好的剧本了吧？ -center
是了，刚刚我所有的发言都是根据玩家的输入，用大语言模型实时生成的。 -center
那么接下来就要给大家揭秘实现原理了。 -center

给没有用过WebGAL的朋友解释一下：WebGAL游戏的运行逻辑是由一系列场景文件控制的。 -center
所有文字、立绘、动画、音效、以及分支跳转，都是由场景文件中写好的的脚本控制。 -center
自然，从一个场景文件也可以跳转到另一个场景文件，这样能方便把多个章节分开，方便管理。 -center

不过大家可能不知道的是，跳转的目标不仅可以是本地的另一个场景文件，也可以是一个链接，表示一个远程的资源或场景文件。 -center

源码里对绝对链接甚至有特殊处理。 -center

同时，这版WebGAL刚好提供了一个获取用户输入的功能，就像这样弹一个输入框 -next
getUserInput:a
客服小祥:然后，就可以把输入嵌入到链接里，发送给我们提前架好的服务器，请求新的场景脚本。 -center
然后服务器拿到嵌入在链接中的输入，和大模型对话后，动态生成包含回复和下一次请求的脚本返回，如此就实现了双向的通信。 -center

原理就是这样，接下来就可以进入实施阶段了。

首先需要一个和大模型通信的后端服务器。

我刚好正在练手写一个LLM客户端，刚写好了一个后端，前端还没开始写。
溜二创的时候忽然想到，WebGAL本身不就是一个巨大的前端吗？|一读源码发现居然可以主动触发远程请求。

这下前后端分离架构了（笑）。

接下来需要写提示词，也就是需要让大模型理解丰川祥子复杂的人设。
这方面我确实也是没什么经验，虽然看起来我想把很多设定一股脑塞进去，但大模型的记忆力显然是有限的。
测试的时候，经常感觉不到【丰川祥子】的特点，而是真的在和一个普通的【客服】对话。
另外这位大祥老师有点藏不住话，经常主动破坏Ave Mujica的世界观。
所以OOC是难以避免的，这点我确实没什么办法。

然后需要一个把大模型输出转换为场景脚本的简单程序。
仅仅是文字的话是比较容易的，不过要加上合适的表情、动作、声音等表现效果就比较困难了。
作为一个动态生成内容的项目，手动去调整各种参数显然并不现实。
然而我们是个AI项目，很多事情可以直接交给大模型一把梭了。
比如动作和表情，我们至少可以通过大模型把回复的喜怒哀乐情感基调定出来。
然后可以预设好的几类情感的演出，随机抽取一个就好了。|效果勉强合格吧，还有微调提升的空间。


语音方面，我目前尝试用了fish-speech这个项目进行音声合成，使用了部分手游提取配音作为参考音源（来自bestdori）
目前还没有进行微调，直接使用原始模型+参考音源进行zeroshot推理。
效果你们也看到了，非常一言难尽。很多中文字是按日语发音给出的，我也无法调整语速、情感方面的信息。
并且目前的实现只能等第一句完全返回后再合成配音，会增加几秒额外的延迟，使用就没那么流畅了。
作为替代，我还尝试了微软TTS（edge-tts），可以配出非常完美的中文配音，除了和大祥老师毫无关联以外没有任何问题。
可以作为备选，毕竟Windows电脑上微软TTS似乎只要装一个Python包就行了。而fish-speech需要配一整套环境，下载好几个GB的模型数据。
当然如果都不喜欢也可以在直接关掉配音。
这方面我完全是小白。如果大家在定制声线的TTS上有心得的话，请不吝赐教。我有空了或许也会试试GPT-SoVITS等别的项目。


最后，为了让有能力的网友们玩上这个项目，我会把当前版本的代码开源，你应该可以在简介或评论区找到github链接。
开源代码里不包含API Key，还请自备。
因为我的后端直接调用了openai库，因此只能用和openai库兼容的模型，比如ChatGPT或者deepseek

总之谢谢大家看到这里。欢迎大家来玩这个项目，如果大家不嫌弃我的代码质量的话也欢迎拿去二次开发，MIT授权。

end